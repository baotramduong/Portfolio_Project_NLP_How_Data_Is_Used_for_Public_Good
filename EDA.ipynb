{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-24T22:51:37.02205Z","iopub.execute_input":"2021-05-24T22:51:37.022457Z","iopub.status.idle":"2021-05-24T22:51:45.551181Z","shell.execute_reply.started":"2021-05-24T22:51:37.022413Z","shell.execute_reply":"2021-05-24T22:51:45.550051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Notebooks that I Adapted:\n\n* https://www.kaggle.com/dineshkumaranbalagan/descriptive-analysis\n\n* https://www.kaggle.com/mlconsult/score-57ish-with-additional-govt-datasets\n\n* https://www.kaggle.com/adnaiksachin25/wordcloud-cosine-jaccard-sequencematcher\n\n* https://www.kaggle.com/ht5brer/difference-between-appearing-and-mentioning\n\n* https://www.kaggle.com/armandmorin/show-us-data\n","metadata":{}},{"cell_type":"markdown","source":"# Introduction\n\n## Goal\n\n* The end goal is to do string matching of known datasets names in order to detect mentions of datasets in scientific publications.\n* To build a strong NLP model that can infer from context whether or not a piece of text in a publication is refering to the usage of a dataset or not.","metadata":{}},{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport random\nimport time\nimport datetime\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS\n\nimport re\nimport json\nfrom tqdm.autonotebook import tqdm\nimport string\n\nimport nltk\nfrom nltk.probability import FreqDist\nfrom nltk.tokenize import word_tokenize\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score\nfrom sklearn.metrics import confusion_matrix, plot_confusion_matrix\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n\nfrom keras.preprocessing import sequence, text\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import utils\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.callbacks import *\n\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU\nfrom keras.layers import Dense, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils.vis_utils import plot_model\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:51:45.552854Z","iopub.execute_input":"2021-05-24T22:51:45.553142Z","iopub.status.idle":"2021-05-24T22:51:54.363508Z","shell.execute_reply.started":"2021-05-24T22:51:45.553115Z","shell.execute_reply":"2021-05-24T22:51:54.362364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Description\n\n         - train.csv- CSV file contains metadata of the publications\n         - train-JSON file contains publications that are referenced in train.csv\n         - test-CSV file contains publications for testing purpose\n         - sample_submission.csv-CSV file conatins publications IDs column and prediction columns\n\n**id** - publication id - note that there are multiple rows for some training documents, indicating multiple mentioned datasets.\n\n**pub_title** -title of the publication (a small number of publications have the same title).\n\n**dataset_title** -the title of the dataset that is mentioned within the publication.\n\n**dataset_label** -a portion of the text that indicates the dataset.\n\n**cleaned_label** -the dataset_label, as passed through the clean_text function from the Evaluation page.\n\n**PredictionString** -To be filled with equivalent of cleaned_label of train data","metadata":{}},{"cell_type":"markdown","source":"# OBTAIN","metadata":{}},{"cell_type":"markdown","source":"### Train Set","metadata":{}},{"cell_type":"code","source":"#define paths\nos.listdir('/kaggle/input/coleridgeinitiative-show-us-the-data/')\ntrain_path = '../input/coleridgeinitiative-show-us-the-data/train'\ntest_path = '../input/coleridgeinitiative-show-us-the-data/test'","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:51:54.365395Z","iopub.execute_input":"2021-05-24T22:51:54.365736Z","iopub.status.idle":"2021-05-24T22:51:54.373232Z","shell.execute_reply.started":"2021-05-24T22:51:54.365699Z","shell.execute_reply":"2021-05-24T22:51:54.372134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#read train data\ntrain_df = pd.read_csv('../input/coleridgeinitiative-show-us-the-data/train.csv')\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:51:54.375074Z","iopub.execute_input":"2021-05-24T22:51:54.375681Z","iopub.status.idle":"2021-05-24T22:51:54.461901Z","shell.execute_reply.started":"2021-05-24T22:51:54.375607Z","shell.execute_reply":"2021-05-24T22:51:54.460702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create a function to get the text from the JSON file and append it to the new column in table\ndef read_json_pub(filename, train_path = train_path, output = 'text'):\n    json_path = os.path.join(train_path, (filename + '.json'))\n    headings = []\n    contents = []\n    combined = []\n    with open(json_path, 'r') as f:\n        json_decode = json.load(f)\n        for data in json_decode:\n            headings.append(data.get('section_title'))\n            contents.append(data.get('text'))\n            combined.append(data.get('section_title'))\n            combined.append(data.get('text'))\n    \n    all_headings = ' '.join(headings)\n    all_contents = ' '.join(contents)\n    all_data = '. '.join(combined)\n    \n    if output == 'text':\n        return all_contents\n    elif output == 'head':\n        return all_headings\n    else:\n        return all_data","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:51:54.463969Z","iopub.execute_input":"2021-05-24T22:51:54.464411Z","iopub.status.idle":"2021-05-24T22:51:54.472383Z","shell.execute_reply.started":"2021-05-24T22:51:54.464361Z","shell.execute_reply":"2021-05-24T22:51:54.471123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#apply the function to train data\ntqdm.pandas()\ntrain_df['text'] = train_df['Id'].progress_apply(read_json_pub)","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:51:54.473767Z","iopub.execute_input":"2021-05-24T22:51:54.47416Z","iopub.status.idle":"2021-05-24T22:52:09.934599Z","shell.execute_reply.started":"2021-05-24T22:51:54.474129Z","shell.execute_reply":"2021-05-24T22:52:09.933697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#recheck\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:52:09.935743Z","iopub.execute_input":"2021-05-24T22:52:09.936017Z","iopub.status.idle":"2021-05-24T22:52:09.949249Z","shell.execute_reply.started":"2021-05-24T22:52:09.935989Z","shell.execute_reply":"2021-05-24T22:52:09.948325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Read Submission Data","metadata":{}},{"cell_type":"code","source":"#read submission data\nsubmission_df = pd.read_csv('../input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\n\n#apply the function to submission data\ntqdm.pandas()\nsubmission_df['text'] = submission_df['Id'].progress_apply(read_json_pub)\n\nsubmission_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:52:09.950508Z","iopub.execute_input":"2021-05-24T22:52:09.950782Z","iopub.status.idle":"2021-05-24T22:52:10.007931Z","shell.execute_reply.started":"2021-05-24T22:52:09.950756Z","shell.execute_reply":"2021-05-24T22:52:10.00707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#save\nsubmission_df.to_csv('submission_df.csv')","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:52:10.010107Z","iopub.execute_input":"2021-05-24T22:52:10.010382Z","iopub.status.idle":"2021-05-24T22:52:10.023751Z","shell.execute_reply.started":"2021-05-24T22:52:10.010355Z","shell.execute_reply":"2021-05-24T22:52:10.022768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Read Samples","metadata":{}},{"cell_type":"code","source":"#let's read the first sample\n\nimport json\nwith open('../input/coleridgeinitiative-show-us-the-data/train/d0fa7568-7d8e-4db9-870f-f9c6f668c17b.json') as f:\n    sample = json.load(f)\n    \nsample[:2]","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:52:10.025552Z","iopub.execute_input":"2021-05-24T22:52:10.025887Z","iopub.status.idle":"2021-05-24T22:52:10.037346Z","shell.execute_reply.started":"2021-05-24T22:52:10.025852Z","shell.execute_reply":"2021-05-24T22:52:10.036355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Within the first section, this publication mentions that they used data from the National Education Longitudinal Study. So the task of this competition is to find string of 'dataset_title' within the 'text' body and return 'cleaned_label'.","metadata":{}},{"cell_type":"code","source":"#get all 'section_title'\nfor s in sample:\n    print(s['section_title'])","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:52:10.038804Z","iopub.execute_input":"2021-05-24T22:52:10.03928Z","iopub.status.idle":"2021-05-24T22:52:10.050345Z","shell.execute_reply.started":"2021-05-24T22:52:10.039238Z","shell.execute_reply":"2021-05-24T22:52:10.049395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('../input/coleridgeinitiative-show-us-the-data/train/2f26f645-3dec-485d-b68d-f013c9e05e60.json') as f:\n    sample = json.load(f)\n    \nsample[:2]","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:52:10.05164Z","iopub.execute_input":"2021-05-24T22:52:10.052063Z","iopub.status.idle":"2021-05-24T22:52:10.062862Z","shell.execute_reply.started":"2021-05-24T22:52:10.052024Z","shell.execute_reply":"2021-05-24T22:52:10.062008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get all 'section_title'\nfor s in sample:\n    print(s['section_title'])","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:52:10.063932Z","iopub.execute_input":"2021-05-24T22:52:10.06442Z","iopub.status.idle":"2021-05-24T22:52:10.074851Z","shell.execute_reply.started":"2021-05-24T22:52:10.064386Z","shell.execute_reply":"2021-05-24T22:52:10.07384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SCRUB","metadata":{}},{"cell_type":"markdown","source":"### Basic Text Cleaning\n\nBefore we can create a bag of words or vectorize each document, we need to clean it up and split each document into an array of individual words. Computers are very particular about strings. If we tokenized our data in its current state, we would run into the following problems:\n\n* Counting things that aren't actually words. \n* Punctuation and capitalization would mess up our word counts. We need to remove punctuation and capitalization, so that all words will be counted correctly.","metadata":{}},{"cell_type":"code","source":"def text_cleaning(text):\n    '''\n    Converts all text to lower case, Removes special charecters, emojis and multiple spaces\n    text - Sentence that needs to be cleaned\n    '''\n    text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\n    text = re.sub(' +', ' ', text)\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               \"]+\", flags = re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:52:10.076076Z","iopub.execute_input":"2021-05-24T22:52:10.076348Z","iopub.status.idle":"2021-05-24T22:52:10.085148Z","shell.execute_reply.started":"2021-05-24T22:52:10.076323Z","shell.execute_reply":"2021-05-24T22:52:10.083868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#clean train text\ntqdm.pandas()\ntrain_df['text'] = train_df['text'].progress_apply(text_cleaning)","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:52:10.08654Z","iopub.execute_input":"2021-05-24T22:52:10.086953Z","iopub.status.idle":"2021-05-24T22:54:20.455799Z","shell.execute_reply.started":"2021-05-24T22:52:10.086922Z","shell.execute_reply":"2021-05-24T22:54:20.454732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#review\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:54:20.457427Z","iopub.execute_input":"2021-05-24T22:54:20.457756Z","iopub.status.idle":"2021-05-24T22:54:20.474717Z","shell.execute_reply.started":"2021-05-24T22:54:20.457713Z","shell.execute_reply":"2021-05-24T22:54:20.473219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#save\ntrain_df.to_csv('train_df.csv')","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:54:20.476396Z","iopub.execute_input":"2021-05-24T22:54:20.476711Z","iopub.status.idle":"2021-05-24T22:54:38.007977Z","shell.execute_reply.started":"2021-05-24T22:54:20.476676Z","shell.execute_reply":"2021-05-24T22:54:38.007087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EXPLORE","metadata":{}},{"cell_type":"code","source":"#train_df = pd.read_csv('../input/flatiron-capstone/train_df.csv', index_col = 0)","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:54:38.009294Z","iopub.execute_input":"2021-05-24T22:54:38.009861Z","iopub.status.idle":"2021-05-24T22:54:38.01314Z","shell.execute_reply.started":"2021-05-24T22:54:38.009826Z","shell.execute_reply":"2021-05-24T22:54:38.012475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get info\ntrain_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:54:38.014376Z","iopub.execute_input":"2021-05-24T22:54:38.014853Z","iopub.status.idle":"2021-05-24T22:54:38.048047Z","shell.execute_reply.started":"2021-05-24T22:54:38.014811Z","shell.execute_reply":"2021-05-24T22:54:38.047287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check null values\ntrain_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:54:38.049471Z","iopub.execute_input":"2021-05-24T22:54:38.049784Z","iopub.status.idle":"2021-05-24T22:54:38.066825Z","shell.execute_reply.started":"2021-05-24T22:54:38.049744Z","shell.execute_reply":"2021-05-24T22:54:38.065807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get summary\ntrain_df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:54:38.068234Z","iopub.execute_input":"2021-05-24T22:54:38.068752Z","iopub.status.idle":"2021-05-24T22:54:38.813942Z","shell.execute_reply.started":"2021-05-24T22:54:38.068714Z","shell.execute_reply":"2021-05-24T22:54:38.812894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Train dataset has 19,661 counts but only 14,316 unique 'Id' in the dataset. This means some 'Id' are duplicates, meaning some 'Id' use multiple datasets.\n\nThe 'pub_title' has 19,661 counts but has only 14,271 unique titles. This means some 'pub_titles' are duplicates. There are less 'pub_title' counts than 'Id' counts, meaning some 'pub_title' has multiple 'Id'.\n\nThe 'dataset_title' has 19,661 counts but has only 45 unique titles. This means some 'dataset_title' are used many times by different publications.\n\nThe 'dataset_label' has 19,661 counts but has only 130 unique labels. This means some 'dataset_label' are duplicates. There are less 'dataset_title' counts than 'dataset_label', meaning some 'dataset_title' are labeled differently by different publications.","metadata":{}},{"cell_type":"code","source":"print('Number of duplicates in Id:', train_df['Id'].duplicated().sum())\nprint('Number of duplicates in pub_title:', train_df['pub_title'].duplicated().sum())\nprint('Number of duplicates in dataset_title:', train_df['dataset_title'].duplicated().sum())\nprint('Number of duplicates in dataset_label:', train_df['dataset_label'].duplicated().sum())\nprint('Number of duplicates in cleaned_label:', train_df['cleaned_label'].duplicated().sum())","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:54:38.815673Z","iopub.execute_input":"2021-05-24T22:54:38.815983Z","iopub.status.idle":"2021-05-24T22:54:38.829004Z","shell.execute_reply.started":"2021-05-24T22:54:38.815951Z","shell.execute_reply":"2021-05-24T22:54:38.827978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check out duplicates\nid_duplicates = train_df['Id'] == 'c754dec7-c5a3-4337-9892-c02158475064'\ntrain_df.loc[id_duplicates][:5]","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:54:38.830197Z","iopub.execute_input":"2021-05-24T22:54:38.830458Z","iopub.status.idle":"2021-05-24T22:54:38.854577Z","shell.execute_reply.started":"2021-05-24T22:54:38.830433Z","shell.execute_reply":"2021-05-24T22:54:38.853332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The same 'Id' and 'pub_title' of sample 4 and 8121 are labeled differently as 'National Education Longitudinal Study' and 'Education Longitudinal Study' although they are the same publication.","metadata":{}},{"cell_type":"code","source":"#check out duplicates\npub_title_duplicates = train_df['pub_title'] == 'Science and Engineering Indicators 2014'\ntrain_df.loc[pub_title_duplicates][:5]","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:54:38.856198Z","iopub.execute_input":"2021-05-24T22:54:38.856484Z","iopub.status.idle":"2021-05-24T22:54:38.931083Z","shell.execute_reply.started":"2021-05-24T22:54:38.856454Z","shell.execute_reply":"2021-05-24T22:54:38.929969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When check out pub_title_duplicates, we see that there are same 'text' with same 'Id' and same 'pub_title' but labeled different i.e 14798 and 14799 dataset_title of 'Survey of Earned Doctorates' are labeled as 'Survey of Earned Doctorates' and 'National Center for Science and Engineering...'","metadata":{}},{"cell_type":"code","source":"#check out duplicates\ndataset_title_duplicates = train_df['dataset_title'] == 'Higher Education Research and Development Survey'\ntrain_df.loc[dataset_title_duplicates][:5]","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:54:38.932452Z","iopub.execute_input":"2021-05-24T22:54:38.932757Z","iopub.status.idle":"2021-05-24T22:54:38.972603Z","shell.execute_reply.started":"2021-05-24T22:54:38.932717Z","shell.execute_reply":"2021-05-24T22:54:38.971701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we see with samples 1450 and 12456, there are same 'text' with same 'Id' and same 'pub_title' but labeled different.","metadata":{}},{"cell_type":"markdown","source":"## Look At Each Feature Individually","metadata":{}},{"cell_type":"markdown","source":"### 'Id'","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (30, 20)),\n\nsns.countplot(y = train_df['Id'], \n              order = train_df['Id'].value_counts(ascending = False)[:20].index, \n              palette = 'Spectral')\nplt.ylabel('Id',fontsize = 20)\nplt.title('Id')\nplt.show()\n\n#save\nplt.savefig('Id.png')","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:54:38.974028Z","iopub.execute_input":"2021-05-24T22:54:38.974298Z","iopub.status.idle":"2021-05-24T22:54:39.416556Z","shell.execute_reply.started":"2021-05-24T22:54:38.974264Z","shell.execute_reply":"2021-05-24T22:54:39.415748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 'pub_title'","metadata":{}},{"cell_type":"code","source":"train_df['pub_title'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:54:39.419684Z","iopub.execute_input":"2021-05-24T22:54:39.419979Z","iopub.status.idle":"2021-05-24T22:54:39.432882Z","shell.execute_reply.started":"2021-05-24T22:54:39.419948Z","shell.execute_reply":"2021-05-24T22:54:39.431916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['pub_title'].value_counts().head(10).to_frame()","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:54:39.434484Z","iopub.execute_input":"2021-05-24T22:54:39.434802Z","iopub.status.idle":"2021-05-24T22:54:39.454831Z","shell.execute_reply.started":"2021-05-24T22:54:39.43476Z","shell.execute_reply":"2021-05-24T22:54:39.453954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create a frequency distribution to see which words are used the most\n\n#define stopwords\nfrom nltk.corpus import stopwords\nstopwords_list = stopwords.words('english') + list(string.punctuation)\nstopwords_list += [\"''\", '\"\"', '...', '``']\n\nwords = list( train_df['pub_title'].values)\nstopwords = stopwords_list\nsplit_words = []\n\nfor word in words:\n    lo_w = []\n    list_of_words = str(word).split()\n    for w in list_of_words:\n        if w not in stopwords:\n            lo_w.append(w)\n    split_words.append(lo_w)\nallwords = []\n\nfor wordlist in split_words:\n    allwords += wordlist\n    \n#get 100 most common words\nmostcommon = FreqDist(allwords).most_common(100)\nmostcommon","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:54:39.456187Z","iopub.execute_input":"2021-05-24T22:54:39.456513Z","iopub.status.idle":"2021-05-24T22:54:40.241606Z","shell.execute_reply.started":"2021-05-24T22:54:39.456484Z","shell.execute_reply":"2021-05-24T22:54:40.240527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot frequency distributions\nwordcloud = WordCloud(width = 1600, height = 800, \n                      background_color = 'black', \n                      colormap = 'Spectral', \n                      stopwords = stopwords_list).generate(str(mostcommon))\n\nfig = plt.figure(figsize = (20, 10), facecolor = 'white')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.title('Top 100 Most Common Words in dataset_title', fontsize = 30)\nplt.tight_layout()\n\n#save\nplt.savefig('pub_title_wordcloud.png')","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:54:40.24307Z","iopub.execute_input":"2021-05-24T22:54:40.243474Z","iopub.status.idle":"2021-05-24T22:54:42.718296Z","shell.execute_reply.started":"2021-05-24T22:54:40.243433Z","shell.execute_reply":"2021-05-24T22:54:42.717487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 'dataset_title'","metadata":{}},{"cell_type":"code","source":"train_df['dataset_title'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:54:42.719398Z","iopub.execute_input":"2021-05-24T22:54:42.71984Z","iopub.status.idle":"2021-05-24T22:54:42.727732Z","shell.execute_reply.started":"2021-05-24T22:54:42.719803Z","shell.execute_reply":"2021-05-24T22:54:42.72673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['dataset_title'].value_counts().head(20).to_frame()","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:54:42.729093Z","iopub.execute_input":"2021-05-24T22:54:42.729398Z","iopub.status.idle":"2021-05-24T22:54:42.74634Z","shell.execute_reply.started":"2021-05-24T22:54:42.729367Z","shell.execute_reply":"2021-05-24T22:54:42.745457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create a frequency distribution to see which words are used the most\nwords = list( train_df['dataset_title'].values)\nstopwords = stopwords_list\nsplit_words = []\n\nfor word in words:\n    lo_w = []\n    list_of_words = str(word).split()\n    for w in list_of_words:\n        if w not in stopwords:\n            lo_w.append(w)\n    split_words.append(lo_w)\nallwords = []\n\nfor wordlist in split_words:\n    allwords += wordlist\n    \n#get 100 most common words\nmostcommon = FreqDist(allwords).most_common(100)\nmostcommon","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:54:42.747725Z","iopub.execute_input":"2021-05-24T22:54:42.74804Z","iopub.status.idle":"2021-05-24T22:54:43.288755Z","shell.execute_reply.started":"2021-05-24T22:54:42.748011Z","shell.execute_reply":"2021-05-24T22:54:43.287777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot frequency distributions\nwordcloud = WordCloud(width = 1600, height = 800, \n                      background_color = 'black', \n                      colormap = 'Spectral', \n                      stopwords = stopwords_list).generate(str(mostcommon))\n\nfig = plt.figure(figsize = (20, 10), facecolor = 'white')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.title('Top 100 Most Common Words in dataset_title', fontsize = 30)\nplt.tight_layout()\n\n#save\nplt.savefig('dataset_title_wordcloud.png')","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:54:43.290103Z","iopub.execute_input":"2021-05-24T22:54:43.29043Z","iopub.status.idle":"2021-05-24T22:54:45.84517Z","shell.execute_reply.started":"2021-05-24T22:54:43.290397Z","shell.execute_reply":"2021-05-24T22:54:45.844065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (30, 30)),\n\nsns.countplot(y = train_df['dataset_title'], \n              order = train_df['dataset_title'].value_counts().index, \n              palette = 'Spectral')\nplt.ylabel('dataset_title',fontsize = 30)\nplt.xticks(fontsize = 30)\nplt.show()\n\n#save\nplt.savefig('dataset_title.png')","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:54:45.846468Z","iopub.execute_input":"2021-05-24T22:54:45.8468Z","iopub.status.idle":"2021-05-24T22:54:46.826942Z","shell.execute_reply.started":"2021-05-24T22:54:45.846769Z","shell.execute_reply":"2021-05-24T22:54:46.825945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 'dataset_label'","metadata":{}},{"cell_type":"code","source":"train_df['dataset_label'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:54:46.828059Z","iopub.execute_input":"2021-05-24T22:54:46.828506Z","iopub.status.idle":"2021-05-24T22:54:46.836486Z","shell.execute_reply.started":"2021-05-24T22:54:46.828463Z","shell.execute_reply":"2021-05-24T22:54:46.835583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['dataset_label'].value_counts().head(20).to_frame()","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:54:46.837801Z","iopub.execute_input":"2021-05-24T22:54:46.838098Z","iopub.status.idle":"2021-05-24T22:54:46.856997Z","shell.execute_reply.started":"2021-05-24T22:54:46.838068Z","shell.execute_reply":"2021-05-24T22:54:46.855733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 'cleaned_label'","metadata":{}},{"cell_type":"code","source":"train_df['cleaned_label'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:54:46.858357Z","iopub.execute_input":"2021-05-24T22:54:46.858723Z","iopub.status.idle":"2021-05-24T22:54:46.874185Z","shell.execute_reply.started":"2021-05-24T22:54:46.858681Z","shell.execute_reply":"2021-05-24T22:54:46.873144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['cleaned_label'].value_counts().head(20).to_frame()","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:54:46.875712Z","iopub.execute_input":"2021-05-24T22:54:46.876087Z","iopub.status.idle":"2021-05-24T22:54:46.890508Z","shell.execute_reply.started":"2021-05-24T22:54:46.876054Z","shell.execute_reply":"2021-05-24T22:54:46.889677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Frequency Distributions","metadata":{}},{"cell_type":"code","source":"#create a frequency distribution to see which words are used the most\nwords = list( train_df['cleaned_label'].values)\nstopwords = stopwords_list\nsplit_words = []\nfor word in words:\n    lo_w = []\n    list_of_words = str(word).split()\n    for w in list_of_words:\n        if w not in stopwords:\n            lo_w.append(w)\n    split_words.append(lo_w)\nallwords = []\nfor wordlist in split_words:\n    allwords += wordlist\n    \n#get 100 most common words\nmostcommon = FreqDist(allwords).most_common(100)\nmostcommon","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:54:46.891983Z","iopub.execute_input":"2021-05-24T22:54:46.892438Z","iopub.status.idle":"2021-05-24T22:54:47.174304Z","shell.execute_reply.started":"2021-05-24T22:54:46.892402Z","shell.execute_reply":"2021-05-24T22:54:47.173417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot frequency distributions\nwordcloud = WordCloud(width = 1600, height = 800, \n                      background_color = 'black', \n                      colormap = 'Spectral', \n                      stopwords = stopwords_list).generate(str(mostcommon))\n\nfig = plt.figure(figsize = (20, 10), facecolor = 'white')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.title('Top 100 Most Common Words in cleaned_label', fontsize = 30)\nplt.tight_layout()\n\n#save\nplt.savefig('cleaned_label_wordcloud.png')","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:54:47.175483Z","iopub.execute_input":"2021-05-24T22:54:47.17577Z","iopub.status.idle":"2021-05-24T22:54:49.918103Z","shell.execute_reply.started":"2021-05-24T22:54:47.175742Z","shell.execute_reply":"2021-05-24T22:54:49.916985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (30, 40)),\n\nsns.countplot(y = train_df['cleaned_label'], \n              order = train_df['cleaned_label'].value_counts().index, \n              palette = 'Spectral')\nplt.ylabel('Cleaned Label',fontsize = 30)\nplt.show()\n\n#save\nplt.savefig('cleaned_label.png')","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:54:49.919698Z","iopub.execute_input":"2021-05-24T22:54:49.920124Z","iopub.status.idle":"2021-05-24T22:54:53.696791Z","shell.execute_reply.started":"2021-05-24T22:54:49.920081Z","shell.execute_reply":"2021-05-24T22:54:53.695748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The classes are highly imbalanced.","metadata":{}},{"cell_type":"markdown","source":"## BiGram\n\nAn n-gram means a sequence of n-words.\n\nSome English words occur together more frequently. So, in a text document we may need to identify such pair of words which will help in sentiment analysis. \n\nBigram is 2 consecutive words in a sentence.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\n#get bigrams \nvectorizer = CountVectorizer(ngram_range = (2, 2))\n\n#matrix of ngrams\nngrams = vectorizer.fit_transform(train_df['cleaned_label']) \nfeatures = (vectorizer.get_feature_names())\nprint('\\n\\nFeatures : \\n', features)\n\n#count frequency of ngrams\nprint('\\n\\nX1 : \\n', ngrams.toarray())\n  \n#apply TFIDF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(ngram_range = (2, 2))\nngrams = vectorizer.fit_transform(train_df['cleaned_label'])\nscores = (ngrams.toarray())\nprint('\\n\\nScores : \\n', scores)\n  \n#get top ranking features\nsums = ngrams.sum(axis = 0)\ndata1 = []\nfor col, term in enumerate(features):\n    data1.append( (term, sums[0,col] ))\nranking = pd.DataFrame(data1, columns = ['term','rank'])\nwords = (ranking.sort_values('rank', ascending = False))\nprint ('\\n\\nWords head : \\n', words.head(60))","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:54:53.69808Z","iopub.execute_input":"2021-05-24T22:54:53.698347Z","iopub.status.idle":"2021-05-24T22:54:53.964461Z","shell.execute_reply.started":"2021-05-24T22:54:53.69832Z","shell.execute_reply":"2021-05-24T22:54:53.963448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#count frequency of ngrams\ncount_values = ngrams.toarray().sum(axis = 0)\n\n#list of ngrams\nvocab = vectorizer.vocabulary_\ndf_bigram = pd.DataFrame(sorted([(count_values[i],k) for k,i in vocab.items()], reverse = True)\n            ).rename(columns = {0: 'frequency', 1: 'bigram'})\n\nplt.figure(figsize = (20, 10))\nsns.lineplot(x = df_bigram['bigram'][:60], y = df_bigram['frequency'][:60])\nplt.xticks(rotation = 90, fontsize = 16)\nplt.xlabel('Bigram',fontsize = 20)\nplt.ylabel('Frequency',fontsize = 20)\nplt.title('Bigram',fontsize = 30)\nplt.show()\n\n#save\nplt.savefig('bigram.png')","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:54:53.965527Z","iopub.execute_input":"2021-05-24T22:54:53.965796Z","iopub.status.idle":"2021-05-24T22:54:54.960819Z","shell.execute_reply.started":"2021-05-24T22:54:53.965771Z","shell.execute_reply":"2021-05-24T22:54:54.959713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TriGram\n\nTrigram is 3 consecutive words in a sentence. ","metadata":{}},{"cell_type":"code","source":"#get trigrams \nvectorizer = CountVectorizer(ngram_range = (3,3))\n\n#matrix of ngrams\nngrams = vectorizer.fit_transform(train_df['cleaned_label']) \nfeatures = (vectorizer.get_feature_names())\nprint('\\n\\nFeatures : \\n', features)\n\n#count frequency of ngrams\nprint('\\n\\nX1 : \\n', ngrams.toarray())\n  \n#apply TFIDF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(ngram_range = (3,3))\nngrams = vectorizer.fit_transform(train_df['cleaned_label'])\nscores = (ngrams.toarray())\nprint('\\n\\nScores : \\n', scores)\n  \n#get top ranking features\nsums = ngrams.sum(axis = 0)\ndata1 = []\nfor col, term in enumerate(features):\n    data1.append( (term, sums[0,col] ))\nranking = pd.DataFrame(data1, columns = ['term','rank'])\nwords = (ranking.sort_values('rank', ascending = False))\nprint ('\\n\\nWords head : \\n', words.head(60))","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:54:54.962258Z","iopub.execute_input":"2021-05-24T22:54:54.962629Z","iopub.status.idle":"2021-05-24T22:54:55.209467Z","shell.execute_reply.started":"2021-05-24T22:54:54.962587Z","shell.execute_reply":"2021-05-24T22:54:55.208445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#count frequency of ngrams\ncount_values = ngrams.toarray().sum(axis = 0)\n\n#list of ngrams\nvocab = vectorizer.vocabulary_\ndf_trigram = pd.DataFrame(sorted([(count_values[i],k) for k,i in vocab.items()], reverse = True)\n            ).rename(columns = {0: 'frequency', 1:'trigram'})\n\nplt.figure(figsize = (20, 10))\nsns.lineplot(x = df_trigram['trigram'][:60], y = df_trigram['frequency'][:60])\nplt.xticks(rotation = 90, fontsize = 16)\nplt.xlabel('Trigram',fontsize = 20)\nplt.ylabel('Frequency',fontsize = 20)\nplt.title('Trigram',fontsize = 30)\nplt.show()\n\n#save\nplt.savefig('trigram.png')","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:54:55.21055Z","iopub.execute_input":"2021-05-24T22:54:55.210816Z","iopub.status.idle":"2021-05-24T22:54:56.37855Z","shell.execute_reply.started":"2021-05-24T22:54:55.210784Z","shell.execute_reply":"2021-05-24T22:54:56.377702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}